{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "strong-timer",
   "metadata": {},
   "source": [
    "# 从磁盘加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imperial-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "surprising-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "   \n",
    "    gpu0 = gpus[2] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-saudi",
   "metadata": {},
   "source": [
    "## 从磁盘加载图像"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-guitar",
   "metadata": {},
   "source": [
    "如果数据太大而无法一次全部放入内存，我们可以使用 tf.data.Dataset 将其从磁盘批量加载到内存中。 该函数可以帮助您为图像数据构建这样一个 tf.data.Dataset。\n",
    "\n",
    "首先，我们下载数据并提取文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "romance-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"  # noqa: E501\n",
    "local_file_path = tf.keras.utils.get_file(\n",
    "    origin=dataset_url, fname=\"image_data\", extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ready-missouri",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/huangwei/.keras/datasets'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The file is extracted in the same directory as the downloaded file.\n",
    "local_dir_path = os.path.dirname(local_file_path)\n",
    "local_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "functioning-parliament",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/huangwei/.keras/datasets'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#local_dir_path = !dirname {local_file_path}\n",
    "#local_dir_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mexican-korean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huangwei/.keras/datasets/image_data\n"
     ]
    }
   ],
   "source": [
    "#!echo  {local_file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "strong-fancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huangwei/.keras/datasets/flower_photos\n"
     ]
    }
   ],
   "source": [
    "# After check mannually, we know the extracted data is in 'flower_photos'.\n",
    "data_dir = os.path.join(local_dir_path, \"flower_photos\")\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-extension",
   "metadata": {},
   "source": [
    "该目录应如下所示。 每个文件夹都包含同一类中的图像。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "after-tourist",
   "metadata": {},
   "source": [
    "flowers_photos/\n",
    "  daisy/\n",
    "  dandelion/\n",
    "  roses/\n",
    "  sunflowers/\n",
    "  tulips/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-boutique",
   "metadata": {},
   "source": [
    "我们可以在加载数据时将数据拆分为训练和测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ignored-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "foster-arcade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2936 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_data = ak.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    # Use 20% data as testing data.\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    # Set seed to ensure the same split when loading testing data.\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "worldwide-investor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 734 files for validation.\n"
     ]
    }
   ],
   "source": [
    "test_data = ak.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-teens",
   "metadata": {},
   "source": [
    "然后我们只做一个 AutoKeras 的快速演示，以确保数据集有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mysterious-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ak.ImageClassifier(overwrite=True, max_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "defensive-happening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 16s]\n",
      "val_loss: 1.268507957458496\n",
      "\n",
      "Best val_loss So Far: 1.268507957458496\n",
      "Total elapsed time: 00h 00m 16s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "92/92 [==============================] - 7s 67ms/step - loss: 2.1336 - accuracy: 0.4319\n",
      "INFO:tensorflow:Assets written to: ./image_classifier/best_model/assets\n"
     ]
    }
   ],
   "source": [
    "clf.fit(train_data, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "proprietary-equity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 1s 49ms/step - loss: 1.1402 - accuracy: 0.5341\n",
      "[1.140238881111145, 0.5340599417686462]\n"
     ]
    }
   ],
   "source": [
    "print(clf.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-apparel",
   "metadata": {},
   "source": [
    "## 从清单加载文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-shark",
   "metadata": {},
   "source": [
    "您还可以以相同的方式加载文本数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "outstanding-detection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 253s 3us/step\n"
     ]
    }
   ],
   "source": [
    "dataset_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "local_file_path = tf.keras.utils.get_file(\n",
    "    fname=\"text_data\",\n",
    "    origin=dataset_url,\n",
    "    extract=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "continent-preview",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/huangwei/.keras/datasets'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The file is extracted in the same directory as the downloaded file.\n",
    "local_dir_path = os.path.dirname(local_file_path)\n",
    "local_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "structural-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After check mannually, we know the extracted data is in 'aclImdb'.\n",
    "data_dir = os.path.join(local_dir_path, \"aclImdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "developing-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the unused data folder.\n",
    "shutil.rmtree(os.path.join(data_dir, \"train/unsup\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-doctor",
   "metadata": {},
   "source": [
    "对于这个数据集，数据已经分为训练和测试。 我们只是分别加载它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "occupied-trash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huangwei/.keras/datasets/aclImdb\n"
     ]
    }
   ],
   "source": [
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "strange-crystal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = ak.text_dataset_from_directory(\n",
    "    os.path.join(data_dir, \"train\"), batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sudden-artist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = ak.text_dataset_from_directory(\n",
    "    os.path.join(data_dir, \"test\"), shuffle=False, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "boxed-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ak.TextClassifier(overwrite=True, max_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "solid-framework",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 01m 15s]\n",
      "val_loss: 0.2936581075191498\n",
      "\n",
      "Best val_loss So Far: 0.2936581075191498\n",
      "Total elapsed time: 00h 01m 15s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/2\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.4334 - accuracy: 0.7804\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 33s 42ms/step - loss: 0.2351 - accuracy: 0.9068\n",
      "INFO:tensorflow:Assets written to: ./text_classifier/best_model/assets\n"
     ]
    }
   ],
   "source": [
    "clf.fit(train_data, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "german-birth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 24s 30ms/step - loss: 0.2710 - accuracy: 0.8921\n",
      "[0.27098336815834045, 0.8921200037002563]\n"
     ]
    }
   ],
   "source": [
    "print(clf.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-artist",
   "metadata": {},
   "source": [
    "## 使用 Python 生成器加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-cream",
   "metadata": {},
   "source": [
    "如果要使用生成器，可以参考以下代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "complicated-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BATCHES = 30\n",
    "BATCH_SIZE = 100\n",
    "N_FEATURES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sustained-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generator(n_batches, batch_size, n_features):\n",
    "    \"\"\"Get a generator returning n_batches random data.\n",
    "\n",
    "    The shape of the data is (batch_size, n_features).\n",
    "    \"\"\"\n",
    "\n",
    "    def data_generator():\n",
    "        for _ in range(n_batches * batch_size):\n",
    "            x = np.random.randn(n_features)\n",
    "            y = x.sum(axis=0) / n_features > 0.5\n",
    "            yield x, y\n",
    "\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "blocked-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(\n",
    "    get_data_generator(N_BATCHES, BATCH_SIZE, N_FEATURES),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=((N_FEATURES,), tuple()),\n",
    ").batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "overall-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ak.StructuredDataClassifier(overwrite=True, max_trials=1, seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tough-football",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 01m 09s]\n",
      "val_accuracy: 0.9490000009536743\n",
      "\n",
      "Best val_accuracy So Far: 0.9490000009536743\n",
      "Total elapsed time: 00h 01m 09s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Assets written to: ./structured_data_classifier/best_model/assets\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x=dataset, validation_data=dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "criminal-channel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 67ms/step - loss: 0.2498 - accuracy: 0.9360\n",
      "[0.24979430437088013, 0.9359999895095825]\n"
     ]
    }
   ],
   "source": [
    "print(clf.evaluate(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
