{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "secure-candle",
   "metadata": {},
   "source": [
    "# 文本回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ready-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-poland",
   "metadata": {},
   "source": [
    "为了使本教程易于理解，我们仅将 IMDB 数据集视为回归数据集。 这意味着我们将 IMDB 数据集的预测目标，即 0 和 1 视为数值，以便它们可以直接用作回归目标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "devoted-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "    gpu0 = gpus[1] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hydraulic-eating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-hours",
   "metadata": {},
   "source": [
    "## 一个简单的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-sword",
   "metadata": {},
   "source": [
    "第一步是准备数据。 这里我们以 IMDB 数据集为例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "immediate-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.utils.get_file(\n",
    "    fname=\"aclImdb.tar.gz\",\n",
    "    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "    extract=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "secondary-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to dataset\n",
    "IMDB_DATADIR = os.path.join(os.path.dirname(dataset), \"aclImdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seventh-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"pos\", \"neg\"]\n",
    "train_data = load_files(\n",
    "    os.path.join(IMDB_DATADIR, \"train\"), shuffle=True, categories=classes\n",
    ")\n",
    "test_data = load_files(\n",
    "    os.path.join(IMDB_DATADIR, \"test\"), shuffle=False, categories=classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "logical-sharing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "b'Zero Day leads you to think, even re-think why two'\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(train_data.data)\n",
    "y_train = np.array(train_data.target)\n",
    "x_test = np.array(test_data.data)\n",
    "y_test = np.array(test_data.target)\n",
    "\n",
    "print(x_train.shape)  # (25000,)\n",
    "print(y_train.shape)  # (25000, 1)\n",
    "print(x_train[0][:50])  # <START> this film was just brilliant casting <UNK>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-monitor",
   "metadata": {},
   "source": [
    "第二步是运行TextRegressor。 作为一个快速演示，我们将 epochs 设置为 2。您还可以为自适应数量的 epochs 保留未指定的 epochs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accompanied-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text regressor.\n",
    "reg = ak.TextRegressor(overwrite=True, max_trials=1)  # It tries 10 different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "disturbed-psychology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 01m 02s]\n",
      "val_loss: 0.15142059326171875\n",
      "\n",
      "Best val_loss So Far: 0.15142059326171875\n",
      "Total elapsed time: 00h 01m 02s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/2\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.1838 - mean_squared_error: 0.1838\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.1230 - mean_squared_error: 0.1230\n",
      "INFO:tensorflow:Assets written to: ./text_regressor/best_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Feed the text regressor with training data.\n",
    "reg.fit(x_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "geological-printer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 16s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict with the best model.\n",
    "predicted_y = reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unauthorized-anchor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 20s 25ms/step - loss: 0.1560 - mean_squared_error: 0.1560\n",
      "[0.1560177057981491, 0.1560177057981491]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model with testing data.\n",
    "print(reg.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-linux",
   "metadata": {},
   "source": [
    "## 验证数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-evanescence",
   "metadata": {},
   "source": [
    "默认情况下，AutoKeras 使用最后 20% 的训练数据作为验证数据。 如下例所示，您可以使用validation_split 来指定百分比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "forced-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    # Split the training data and use the last 15% as validation data.\n",
    "    validation_split=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-efficiency",
   "metadata": {},
   "source": [
    "您还可以使用自己的验证集，而不是使用validation_data 将其从训练数据中分离出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "female-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 5000\n",
    "x_val = x_train[split:]\n",
    "y_val = y_train[split:]\n",
    "x_train = x_train[:split]\n",
    "y_train = y_train[:split]\n",
    "reg.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    # Use your own validation set.\n",
    "    validation_data=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-quantity",
   "metadata": {},
   "source": [
    "## 自定义搜索空间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-eclipse",
   "metadata": {},
   "source": [
    "对于高级用户，您可以使用 AutoModel 而不是 TextRegressor 来自定义您的搜索空间。 您可以为一些高级配置配置 TextBlock，例如，针对要使用的文本矢量化方法类型的矢量化器。 您可以使用“sequence”，它使用 TextToInteSequence 将单词转换为整数并使用 Embedding 嵌入整数序列，或者您可以使用“ngram”，它使用 TextToNgramVector 对句子进行向量化。 您也可以不指定这些参数，这将使不同的选择自动调整。 有关详细信息，请参阅以下示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "distributed-faculty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 14s]\n",
      "val_loss: 0.36922892928123474\n",
      "\n",
      "Best val_loss So Far: 0.36922892928123474\n",
      "Total elapsed time: 00h 00m 14s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/2\n",
      "157/157 [==============================] - 5s 29ms/step - loss: 0.5417 - mean_squared_error: 0.5417\n",
      "Epoch 2/2\n",
      "157/157 [==============================] - 5s 29ms/step - loss: 0.4120 - mean_squared_error: 0.4120\n",
      "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
     ]
    }
   ],
   "source": [
    "input_node = ak.TextInput()\n",
    "output_node = ak.TextBlock(block_type=\"ngram\")(input_node)\n",
    "output_node = ak.RegressionHead()(output_node)\n",
    "reg = ak.AutoModel(\n",
    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
    ")\n",
    "reg.fit(x_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-friday",
   "metadata": {},
   "source": [
    "AutoModel 的用法类似于 Keras 的函数式 API。 基本上，您正在构建一个图，其边是块，节点是块的中间输出。 使用 output_node = ak.[some_block]([block_args])(input_node) 添加从 input_node 到 output_node 的边。\n",
    "\n",
    "您甚至还可以使用更细粒度的块来进一步自定义搜索空间。 请参阅以下示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "expected-swedish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 15s]\n",
      "val_loss: 0.19610105454921722\n",
      "\n",
      "Best val_loss So Far: 0.19610105454921722\n",
      "Total elapsed time: 00h 00m 15s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/2\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.2793 - mean_squared_error: 0.2793\n",
      "Epoch 2/2\n",
      "157/157 [==============================] - 6s 38ms/step - loss: 0.2067 - mean_squared_error: 0.2067\n",
      "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
     ]
    }
   ],
   "source": [
    "input_node = ak.TextInput()\n",
    "output_node = ak.TextToIntSequence()(input_node)\n",
    "output_node = ak.Embedding()(output_node)\n",
    "# Use separable Conv layers in Keras.\n",
    "output_node = ak.ConvBlock(separable=True)(output_node)\n",
    "output_node = ak.RegressionHead()(output_node)\n",
    "reg = ak.AutoModel(\n",
    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
    ")\n",
    "reg.fit(x_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-eight",
   "metadata": {},
   "source": [
    "## 数据格式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-hydrogen",
   "metadata": {},
   "source": [
    "AutoKeras TextRegressor 对于数据格式非常灵活。\n",
    "\n",
    "对于文本，输入数据应该是一维的，对于回归目标，应该是数值向量。 AutoKeras 接受 numpy.ndarray。\n",
    "\n",
    "我们还支持对训练数据使用 tf.data.Dataset 格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "arctic-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,))).batch(32)\n",
    "test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "gorgeous-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 00m 14s]\n",
      "val_loss: 0.19924014806747437\n",
      "\n",
      "Best val_loss So Far: 0.19136081635951996\n",
      "Total elapsed time: 00h 00m 27s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/2\n",
      "157/157 [==============================] - 6s 33ms/step - loss: 0.2826 - mean_squared_error: 0.2826\n",
      "Epoch 2/2\n",
      "157/157 [==============================] - 5s 33ms/step - loss: 0.2010 - mean_squared_error: 0.2010\n",
      "INFO:tensorflow:Assets written to: ./text_regressor/best_model/assets\n"
     ]
    }
   ],
   "source": [
    "reg = ak.TextRegressor(overwrite=True, max_trials=2)\n",
    "# Feed the tensorflow Dataset to the regressor.\n",
    "reg.fit(train_set, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "happy-diana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 16s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict with the best model.\n",
    "predicted_y = reg.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acceptable-design",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 20s 26ms/step - loss: 0.1985 - mean_squared_error: 0.1985\n",
      "[0.19846472144126892, 0.19846472144126892]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model with testing data.\n",
    "print(reg.evaluate(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-definition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
